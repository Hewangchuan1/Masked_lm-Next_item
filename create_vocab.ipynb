{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个代码用来读取所有视频基于hist5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/50000/hist50000.csv', names=['userid','videoid','time','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = data.videoid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/vocab.txt', 'w',encoding='UTF-8') as f:\n",
    "    for i in video:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个代码用来读取用户的行为基于hist5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/50000/hist50000.csv', names=['userid','videoid','time','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_behavior = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.userid.unique().tolist():\n",
    "    user_behavior[i] = data[data.userid==i].videoid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = list(user_behavior.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单个用户看过的最大的视频数量为： 298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "for i in behavior:\n",
    "    if len(i)>=num:\n",
    "        num=len(i)\n",
    "print('单个用户看过的最大的视频数量为：',num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/user_behavior.txt', 'w',encoding='UTF-8') as f:\n",
    "    f.write(str(user_behavior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/behavior.txt', 'w',encoding='UTF-8') as f:\n",
    "    f.write(str(list(user_behavior.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个代码用来产生的是用来产生预训练数据的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/behavior.txt', 'r',encoding='UTF-8') as f:\n",
    "    behavior = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_behavior = []\n",
    "for i in range(len(behavior)):\n",
    "    pre_train_behavior.append(behavior[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/pre_train_behavior.txt', 'w',encoding='UTF-8') as f:\n",
    "    f.write(str(pre_train_behavior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这个代码用来产生训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "rng = random.Random(12345)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = './Data/50000/behavior.txt'\n",
    "vocab_file = './Data/50000/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(input_file, 'r') as f:\n",
    "    data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if len(data[i]) > 128:\n",
    "        data[i] = data[i][-128:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_file, 'r', encoding='UTF-8') as f:\n",
    "    vocab_ls = f.read()\n",
    "    vocab_ls = vocab_ls.strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer():\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dt = self.reflect_vocab()\n",
    "        \n",
    "    def reflect_vocab(self):\n",
    "        self.vocab_dt = collections.OrderedDict()\n",
    "        index = 0\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.vocab_dt[self.vocab[i]] = index\n",
    "            index +=1\n",
    "        return self.vocab_dt\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab_dt[token])\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = list(tokenizer.vocab_dt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_pos = []\n",
    "data_all_neg = []\n",
    "data_all = []\n",
    "for i in range(len(data)):\n",
    "    data_pos = data[i]\n",
    "    data_neg = data[i]\n",
    "    while True:\n",
    "        random_index = rng.randint(0, len(tokenizer.vocab_dt.keys()))\n",
    "        if vocab_words[random_index] not in data_pos:\n",
    "            break\n",
    "    data_neg[-1] = vocab_words[random_index]\n",
    "    #打包正负样本\n",
    "    data_all_pos.append(['1', data_pos])\n",
    "    data_all_neg.append(['0', data_neg])\n",
    "    data_all.append(['1', data_pos])\n",
    "    data_all.append(['0', data_neg])\n",
    "rng.shuffle(data_all)\n",
    "rng.shuffle(data_all_pos)\n",
    "rng.shuffle(data_all_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_pos_DF = pd.DataFrame(data_all_pos, columns=['label', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_pos_DF.to_csv('./Data/50000/data_all_pos_DF.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_neg_DF = pd.DataFrame(data_all_neg, columns=['label', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_neg_DF.to_csv('./Data/50000/data_all_neg_DF.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.DataFrame(data_all, columns=['label', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('./Data/50000/data_all.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, dfTest = train_test_split(data_all, random_state=12345, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('./Data/50000/dfTrain.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest.to_csv('./Data/50000/dfTest.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.Open('./Data/50000/dfTrain.csv', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,line) in enumerate(lines):\n",
    "    if i==0:\n",
    "        continue\n",
    "    label = eval(line[0])\n",
    "    text_a = eval(line[1])[:-1]\n",
    "    next_item = eval(line[1])[-1]\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把长度限制在129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_file = './Data/50000/behavior.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r') as f:\n",
    "    data = eval(f.read())\n",
    "for i in range(len(data)):\n",
    "    if len(data[i])>129:\n",
    "        data[i] = data[i][-129:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/129_behavior.txt', 'w') as f:\n",
    "    f.write(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 产生next_item的训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_file = './Data/50000/129_behavior.txt'\n",
    "vocab_file = './Data/50000/vocab.txt'\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r') as f:\n",
    "    data = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_file, 'r', encoding='UTF-8') as f:\n",
    "    vocab_ls = f.read()\n",
    "    vocab_ls = vocab_ls.strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "for index in range(len(data)):\n",
    "    pos_list = data[index]\n",
    "    def gen_neg():\n",
    "        neg = pos_list[0]\n",
    "        while neg in pos_list:\n",
    "            neg = random.choice(vocab_ls)\n",
    "        return neg\n",
    "    neg_list = [gen_neg() for i in range(len(pos_list))]\n",
    "    \n",
    "    for i in range(1, len(pos_list)):\n",
    "        hist_i = pos_list[:i]\n",
    "        if i != len(pos_list) -1:\n",
    "            train_set.append((hist_i, pos_list[i], 1))\n",
    "            train_set.append((hist_i, neg_list[i], 0))\n",
    "        else:\n",
    "            label = (pos_list[i], neg_list[i])\n",
    "            test_set.append((hist_i, label))\n",
    "            \n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/train_set.txt', 'w', encoding='UTF-8') as f:\n",
    "    f.write(str(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/test_set.txt', 'w', encoding='UTF-8') as f:\n",
    "    f.write(str(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/50000/repeat/repeat_train_set.txt', 'w', encoding='UTF-8') as f:\n",
    "    f.write(str(train_set[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有需要就运行的调试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "with open('./Data/50000/train_set.txt', 'r', encoding='UTF-8') as f:\n",
    "    while True:\n",
    "        data = f.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        print(len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(data)):\n",
    "    a.append(len(data[i][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
